{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from wit import Wit\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "#from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Selected_comments_with_witai_sentiment_results\n",
    "## import comment with side effects for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_tem_l400 = pd.read_csv(\"wit_ai_sentment/SE_tem_l400.csv\",index_col=0,header=None)[1]\n",
    "SE_tem_l400 = SE_tem_l400.iloc[3:].astype(str)\n",
    "idxs = list(SE_tem_l400.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_quest2 = pd.read_csv(\"wit_ai_sentment/SE_tem_l400_first_request.ccv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 30689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "value\n",
       "negative    16765\n",
       "neutral     12010\n",
       "positive     1914\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total results:\",len(first_quest2))\n",
    "first_quest2.groupby('value').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is now a year down the road and the medicine no longer gives me negative side effects. However, one glaring side effect of this medicine is impotence and some aphrodisiac should always be prescribed to restore sexual function to the one who uses this medicine\n",
      "\n",
      "I haven't noticed any side effects except maybe that I'm a bit hungrier than usual -- for instance, I'll feel hungry and get a meal but not be able to finish it\n",
      "\n",
      "No negative side effects for me either\n",
      "\n",
      "Don't stop taking it if you have no side effects\n",
      "\n",
      "The only bad side effect that has stuck is feeling extremely tired early in the evenings and I usually fall asleep watching tv on the couch at about 9pm\n",
      "\n",
      "I was completely migraine free for approximately 2 1/2 yrs while taking this medicine, I had no side effects really, maybe gained a few pounds maybe 4 or 5 during this 2 year stretch,  then all of a sudden the migraines started back again about 1 month age\n",
      "\n",
      "Just because the side effects may go away doesn't make it OK\n",
      "\n",
      "I haven't had horrid side effects yet\n",
      "\n",
      "Those negative side effects went away after 3 weeks\n",
      "\n",
      "This drug is very affective but the side effects are kinda crappy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i_n in first_quest2[first_quest2['value'] == 'negative'].sample(10)['text']:#.iloc[0]\n",
    "    print(i_n+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t like the side effects at all, shaking, racing heart&hellip\n",
      "\n",
      "I have been on it for 6 days now and have had no real side effects\n",
      "\n",
      "I have experienced so many body changes that it urged me to look up the side effects\n",
      "\n",
      "I have no side effects with this pill, other than running to the restroom a bit more during the first couple of days I was taking the medication\n",
      "\n",
      "Another interesting side effect - I used to get ear infections and strep throat on a regular basis until starting this\n",
      "\n",
      "Works well, no side effects\n",
      "\n",
      "No side effects any more\n",
      "\n",
      "I felt much the same as I did on the Celexa without the side effects.  I started taking my 20 mg dose every other day to reduce the side effects but ended up with swelling of my face and tongue\n",
      "\n",
      "Still no side effects so they moved me up to the highest dosage\n",
      "\n",
      "No major side effects at all\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i_n in first_quest2[first_quest2['value'] == 'neutral'].sample(10)['text']:#.iloc[0]\n",
    "    print(i_n+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've not had any side effects that I am aware of yet.  If this stays effective and no side effects emerge, my life has changed wonderfully forever.  Also, I pray that nothing occurs where this shows over time to have any negative side effects\n",
      "\n",
      "I have minor side effects and my energy level has been great\n",
      "\n",
      "I had minor side effects of nausea and mood swings.  Wellbutrin is an amazing medication, I recommend putting up with the side effects because the results are life changing\n",
      "\n",
      "I was switched onto this drug after taking lorazepam (Ativan) which I was very happy with and had almost no side effects with\n",
      "\n",
      "If it wasn't for this side effect, it works great for PLMD\n",
      "\n",
      "Other side effects I've noticed are.  I so glad to know it may be a side effect of Pristiq\n",
      "\n",
      "I have loved the convenience of just taking one pill every 24hrs and have never had any side effects at all\n",
      "\n",
      "I experienced no side effects with any of the medications and am overall happy with the results\n",
      "\n",
      "I have experienced NO side effects at all, in fact the skin on my face is also glowing and radiant\n",
      "\n",
      "The side effects have subsided and I'm so thankful to be anxiety free even though it took almost 4 months\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i_n in first_quest2[first_quest2['value'] == 'positive'].sample(10)['text']:#.iloc[0]\n",
    "    print(i_n+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the model using only these comment with side effects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "#from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "## only need to remove punctuation and stemize\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove special symbol\n",
    "def rm_sym(df):\n",
    "    df['review'] = df['review'].str.replace(\"&#039;\",'\\'')\n",
    "    df['review'].head()\n",
    "    df['rating_cate'] = ''\n",
    "    df.loc[df['rating'] >= 7,'rating_cate'] = 'high'\n",
    "    df.loc[df['rating'] <= 4,'rating_cate'] = 'low'\n",
    "    df.loc[(df['rating'] > 4) & (df['rating'] < 7),'rating_cate'] = 'medium'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('drugsCom_raw/drugsComTrain_raw.tsv',sep='\\t',index_col=0)\n",
    "df = rm_sym(df)\n",
    "\n",
    "\n",
    "test = pd.read_csv(\"drugsCom_raw/drugsComTest_raw.tsv\",sep='\\t', index_col=0)\n",
    "test = rm_sym(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "train = pd.concat([df.loc[first_quest2.index], first_quest2],1)\n",
    "## Generate table of words with their counts\n",
    "con_vec = TfidfVectorizer(stop_words='english',tokenizer=tokenize)\n",
    "X_train_p = con_vec.fit_transform(train['review'])\n",
    "#y_train = train['rating_cate']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30689,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negnornot = (train['value'] == \"negative\").astype(int)\n",
    "negnornot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test =  train_test_split(X_train_p,negnornot,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68886861, 0.67961418, 0.6835245 ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l1',multi_class='auto',solver='saga')\n",
    "#lr.fit(X,target_3.iloc[:,0])\n",
    "\n",
    "cv_score = cross_val_score(lr,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7632951 , 0.77111575, 0.7653806 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=300,n_jobs=-1)\n",
    "rfc_cv_score = cross_val_score(rfc,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "rfc_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8118076371692949"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=300,n_jobs=-1)\n",
    "rfc.fit(X_train,y_train)\n",
    "y_test_predict = rfc.predict(X_test)\n",
    "accuracy_score(y_test,y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>0.011099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>effect</th>\n",
       "      <td>0.009980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>headach</th>\n",
       "      <td>0.005513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horribl</th>\n",
       "      <td>0.005404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0.005260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>0.005085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onli</th>\n",
       "      <td>0.004601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>0.004482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week</th>\n",
       "      <td>0.004296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0.004151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terribl</th>\n",
       "      <td>0.004109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nausea</th>\n",
       "      <td>0.004071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst</th>\n",
       "      <td>0.003929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <td>0.003844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ani</th>\n",
       "      <td>0.003821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drug</th>\n",
       "      <td>0.003786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>0.003616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.003519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>0.003505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>im</th>\n",
       "      <td>0.003480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pain</th>\n",
       "      <td>0.003458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medic</th>\n",
       "      <td>0.003422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sever</th>\n",
       "      <td>0.003422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ive</th>\n",
       "      <td>0.003408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veri</th>\n",
       "      <td>0.003382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.003339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>becaus</th>\n",
       "      <td>0.003296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>0.003292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>just</th>\n",
       "      <td>0.003284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tri</th>\n",
       "      <td>0.003276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stsys</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stubbi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incivek</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incit</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incidens</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incd</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10377</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inadequaci</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inactivesugar</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>improvedbut</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stugeron</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>improp</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157101</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impost</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>implod</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immunologist</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immunoligist</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immigran</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immensli</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immediatley</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immeasur</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immacul</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imitrix</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imbalanceacn</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imatinib</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>illinoi</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>illeum</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15ish</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>il2</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foreverrr</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24462 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "bad            0.011099\n",
       "effect         0.009980\n",
       "headach        0.005513\n",
       "horribl        0.005404\n",
       "year           0.005260\n",
       "work           0.005085\n",
       "onli           0.004601\n",
       "day            0.004482\n",
       "week           0.004296\n",
       "month          0.004151\n",
       "terribl        0.004109\n",
       "nausea         0.004071\n",
       "worst          0.003929\n",
       "start          0.003844\n",
       "ani            0.003821\n",
       "drug           0.003786\n",
       "feel           0.003616\n",
       "great          0.003519\n",
       "time           0.003505\n",
       "im             0.003480\n",
       "pain           0.003458\n",
       "medic          0.003422\n",
       "sever          0.003422\n",
       "ive            0.003408\n",
       "veri           0.003382\n",
       "like           0.003339\n",
       "becaus         0.003296\n",
       "help           0.003292\n",
       "just           0.003284\n",
       "tri            0.003276\n",
       "...                 ...\n",
       "stsys          0.000000\n",
       "stubbi         0.000000\n",
       "incivek        0.000000\n",
       "incit          0.000000\n",
       "incidens       0.000000\n",
       "incd           0.000000\n",
       "10377          0.000000\n",
       "inadequaci     0.000000\n",
       "inactivesugar  0.000000\n",
       "improvedbut    0.000000\n",
       "stugeron       0.000000\n",
       "improp         0.000000\n",
       "157101         0.000000\n",
       "impost         0.000000\n",
       "implod         0.000000\n",
       "immunologist   0.000000\n",
       "immunoligist   0.000000\n",
       "immigran       0.000000\n",
       "immensli       0.000000\n",
       "immediatley    0.000000\n",
       "immeasur       0.000000\n",
       "immacul        0.000000\n",
       "imitrix        0.000000\n",
       "imbalanceacn   0.000000\n",
       "imatinib       0.000000\n",
       "illinoi        0.000000\n",
       "illeum         0.000000\n",
       "15ish          0.000000\n",
       "il2            0.000000\n",
       "foreverrr      0.000000\n",
       "\n",
       "[24462 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(rfc.feature_importances_,con_vec.get_feature_names())\n",
    "feature_importances.sort_values(0,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69721064, 0.68847758, 0.69890511])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=300)\n",
    "gbc_cv_score = cross_val_score(gbc,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "gbc_cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d3d0a1d3284a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msvm_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'poly'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgbc_cv_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_poly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgbc_cv_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 240\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_sparse_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshrinking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 random_seed)\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/svm/libsvm_sparse.pyx\u001b[0m in \u001b[0;36msklearn.svm.libsvm_sparse.libsvm_sparse_train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m\"\"\"base matrix class for compressed row and column oriented matrices\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0m_data_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "svm_poly = SVC(kernel='poly')\n",
    "gbc_cv_score = cross_val_score(svm_poly,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "gbc_cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_lin = SVC(kernel='linear')\n",
    "gbc_cv_score = cross_val_score(svm_lin,X_train,y_train,scoring='accuracy',cv=3,n_jobs=-1)\n",
    "gbc_cv_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "#import requests\n",
    "%matplotlib inline\n",
    "from sklearn.svm import SVC\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "## remove special symbol\n",
    "\n",
    "def clean_text(df_tem3):\n",
    "    df_tem3['review'] = df_tem3['review'].str.replace(\"&#039;\",'\\'')\n",
    "    df_tem3['review'] = df_tem3['review'].str.replace(\"\\\"\",\"\").str.lower()\n",
    "    df_tem3['review'] = df_tem3['review'].str.replace( r\"(\\\\r)|(\\\\n)|(\\\\t)|(\\\\f)|(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(&#039;)|(\\d\\s)|(\\d)|(\\/)\",\"\")\n",
    "    df_tem3['review'] = df_tem3['review'].str.replace(\"\\\"\",\"\").str.lower()\n",
    "    df_tem3['review'] = df_tem3['review'].str.replace( r\"(\\$)|(\\-)|(\\\\)|(\\s{2,})\",\" \")\n",
    "    df_tem3['review'].sample(1).iloc[0]\n",
    "\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    df_tem3['review'] = df_tem3['review'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split(\" \")]))\n",
    "    return df_tem3\n",
    "\n",
    "\n",
    "np.random.seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow \n",
    "\n",
    "#from tensorflow import tensorflow.keras\n",
    "\n",
    "#from keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPool1D \n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "\n",
    "MAX_NB_WORDS = 500\n",
    "max_review_length = 500\n",
    "EMBEDDING_DIM = 160\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_quest3 = first_quest2.copy()\n",
    "first_quest3.columns = ['review', 'confidence', 'value']\n",
    "df_tem3 = clean_text(first_quest3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df_tem3,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the data\n",
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
    "                      lower=True, split=' ', char_level=False, \n",
    "                      oov_token=None, document_count=0)\n",
    "\n",
    "tokenizer.fit_on_texts(train['review'])\n",
    "train_sequences = tokenizer.texts_to_sequences(train['review'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "X_train = sequence.pad_sequences(train_sequences, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(test_sequences, maxlen = max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24551, 500) <-- shape of train_data ready for val/train split.\n",
      "(6138, 500) <-- shape of final_test_data ready for fedding to network.\n",
      "6232 <-- Length of Word Index\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(X_train.shape, '<-- shape of train_data ready for val/train split.')\n",
    "print(X_test.shape, '<-- shape of final_test_data ready for fedding to network.')\n",
    "print(len(tokenizer.word_index), '<-- Length of Word Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24551,) <-- shape of train_data ready for val/train split.\n",
      "(6138,) <-- shape of final_test_data ready for fedding to network.\n"
     ]
    }
   ],
   "source": [
    "y_train = (train['value'] == 'negative').astype(int)\n",
    "y_test = (test['value'] == 'negative').astype(int)\n",
    "print(y_train.shape, '<-- shape of train_data ready for val/train split.')\n",
    "print(y_test.shape, '<-- shape of final_test_data ready for fedding to network.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words  = min(MAX_NB_WORDS, len(word_index))\n",
    "lstm_out = max_review_length\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words,EMBEDDING_DIM,input_length=max_review_length))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "## add conv using kernal No.32 and size 3x3, actiation='relu'(rm neg)\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(MaxPool1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(50, return_sequences=True)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(50)))\n",
    "#model.add(Attention(max_review_length))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "## one-code mutiple categories targets use 'categorical_crossentropy' not 'binary_crossentropy'\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 160)          80000     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 500, 100)          84400     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 224,901\n",
      "Trainable params: 224,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/ubuntu/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19640 samples, validate on 4911 samples\n",
      "Epoch 1/40\n",
      "19640/19640 [==============================] - 541s 28ms/step - loss: 0.4063 - acc: 0.8113 - val_loss: 0.3210 - val_acc: 0.8666\n",
      "Epoch 2/40\n",
      "19640/19640 [==============================] - 525s 27ms/step - loss: 0.3189 - acc: 0.8674 - val_loss: 0.3047 - val_acc: 0.8754\n",
      "Epoch 3/40\n",
      "19640/19640 [==============================] - 522s 27ms/step - loss: 0.2900 - acc: 0.8817 - val_loss: 0.2933 - val_acc: 0.8841\n",
      "Epoch 4/40\n",
      "19640/19640 [==============================] - 529s 27ms/step - loss: 0.2674 - acc: 0.8910 - val_loss: 0.2850 - val_acc: 0.8847\n",
      "Epoch 5/40\n",
      "19640/19640 [==============================] - 525s 27ms/step - loss: 0.2489 - acc: 0.8980 - val_loss: 0.2893 - val_acc: 0.8817\n",
      "Epoch 6/40\n",
      "19640/19640 [==============================] - 527s 27ms/step - loss: 0.2359 - acc: 0.9049 - val_loss: 0.3160 - val_acc: 0.8735\n",
      "Epoch 7/40\n",
      "19640/19640 [==============================] - 528s 27ms/step - loss: 0.2203 - acc: 0.9119 - val_loss: 0.3031 - val_acc: 0.8847\n",
      "Epoch 8/40\n",
      "19640/19640 [==============================] - 525s 27ms/step - loss: 0.2107 - acc: 0.9175 - val_loss: 0.2993 - val_acc: 0.8811\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "## if 4 steps without decreasing of loss in valid set, stop the trainning\n",
    "batch = 32 \n",
    "epoch = 40\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 4)\n",
    "\n",
    "LSTM_model = model.fit(X_train, y_train, batch_size=batch, epochs=epoch,\n",
    "                       validation_split=0.2,callbacks=[early_stopping], shuffle = True)\n",
    "\n",
    "best_score = min(LSTM_model.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model_history = pd.DataFrame(LSTM_model.history)\n",
    "file_name = \"Wit_ai_\" + \"bilstm_2l.csv\"\n",
    "LSTM_model_history.to_csv('./model_report/' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model.save('./model_report/Wit_ai_bilstm_2l.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_amazonei_tensorflow_p36)",
   "language": "python",
   "name": "conda_amazonei_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
